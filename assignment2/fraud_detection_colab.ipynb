{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kxn9fj4YUPuB"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q0KHqQBXUag0"
   },
   "outputs": [],
   "source": [
    "def encode_labels(train, test, column):\n",
    "    # Merge train and test before factorization\n",
    "    # This is to ensure that we don't encode labels\n",
    "    # with different codes for test and train.\n",
    "    \n",
    "    all_labels, _ = pd.concat([train[column], test[column]], axis=0).factorize(sort=True)\n",
    "    \n",
    "    train[column] = all_labels[:len(train)].astype(int)\n",
    "    test[column] = all_labels[len(train):].astype(int)\n",
    "\n",
    "def frequency_encode(train, test, columns):\n",
    "    # Apparently this gives good results in lots of kaggle competitions\n",
    "    # I couldn't find much online why the frequency is actually useful\n",
    "    # as a feature. My intuition is that the model might match low or\n",
    "    # high frequencies with card fraud in a case or another. Eg. if \n",
    "    # most of the frauds are coming from a certain email domain, the\n",
    "    # frequency is going to be a useful feature.\n",
    "    n = len(train) + len(test)\n",
    "    for column in columns:\n",
    "        f = (pd.concat(\n",
    "                [train[column], test[column]], axis=0\n",
    "            ).value_counts(dropna=True)/n).to_dict()    \n",
    "        train[f\"{column}_FE\"] = train[column].map(f).astype(float)\n",
    "        test[f\"{column}_FE\"] = test[column].map(f).astype(float)\n",
    "\n",
    "def concatenate(train, test, col1, col2):\n",
    "    # Concatenate two columns together\n",
    "    # The combination of two columns might define unique features.\n",
    "    # Eg. A person can be generally identified by name and dob.\n",
    "    # Similarly, card related features can be identified by \n",
    "    # various combination of given features.\n",
    "    # Address, card date and email domain might define a credit card.\n",
    "    # Multiple transactions can be part of the same credit card.\n",
    "    new_name = f\"{col1}_{col2}\"\n",
    "    train[new_name] = train[col1].astype(str)+ \"_\" + train[col2].astype(str)\n",
    "    test[new_name] = test[col1].astype(str) + \"_\" + test[col2].astype(str)\n",
    "    \n",
    "    encode_labels(train, test, new_name)\n",
    "    \n",
    "\n",
    "def groupby(train, test, indexes, columns, aggregation):\n",
    "    # We want for example, the mean transaction value for every card1_address1 pair\n",
    "    for index in indexes:\n",
    "        for column in columns:\n",
    "            new_name = f\"{column}_{index}_{aggregation}\"\n",
    "\n",
    "            f = pd.concat([train[[index, column]], test[[index, column]]])\n",
    "            f = f.groupby([index])[column].agg(\n",
    "                [aggregation]\n",
    "            ).reset_index().rename(columns={aggregation: new_name})\n",
    "            f.index = list(f[index])\n",
    "            f = f[new_name].to_dict()\n",
    "            train[new_name] = train[index].map(f).astype(float)\n",
    "            test[new_name] = test[index].map(f).astype(float)\n",
    "            train[new_name].fillna(-1, inplace=True)\n",
    "            test[new_name].fillna(-1, inplace=True)\n",
    "\n",
    "def groupby_unique(train, test, columns, card_id):\n",
    "    # Every card_id identifies a card\n",
    "    # but generally a card_id does transactions only from a \n",
    "    # handful of email addresses, locations and details as \n",
    "    # such. So it's worth creating features that contain\n",
    "    # the number of unique email addresses, addresses, distances, etc\n",
    "    # If a card has distances (dist1) very high, might be a good\n",
    "    # proxy for fraudulent transactions.\n",
    "    \n",
    "    for column in columns:\n",
    "        f = pd.concat([train[[card_id, column]], test[[card_id, column]]], axis=0)\n",
    "        grouped = f.groupby(card_id)[column].agg([\"nunique\"])[\"nunique\"].to_dict()\n",
    "        train[f\"{column}_{card_id}_count\"] = train[card_id].map(grouped).astype(\"float32\")\n",
    "        test[f\"{column}_{card_id}_count\"] = test[card_id].map(grouped).astype(\"float32\")\n",
    "\n",
    "\n",
    "def factorize(train, test): \n",
    "    # Factorizes any object or categorical columns.\n",
    "    for column in test.columns:\n",
    "        if str(train[column].dtype) not in {\"category\", \"object\"}:\n",
    "            continue\n",
    "        encode_labels(train, test, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SuUSNd9HUb-8"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\n",
    "    \"/content/drive/My Drive/train_transaction.csv\", index_col=\"TransactionID\")\n",
    "X_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/test_transaction.csv\", index_col=\"TransactionID\")\n",
    "test_identity = pd.read_csv(\n",
    "    \"/content/drive/My Drive/test_identity.csv\", index_col=\"TransactionID\")\n",
    "train_identity = pd.read_csv(\n",
    "    \"/content/drive/My Drive/train_identity.csv\", index_col=\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O0-gY4tWJAQg"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "X_test = X_test.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# this does decref so GC can pick them up\n",
    "# helps having more free ram and less swap on disk\n",
    "# if potentially all the ram is used (although )\n",
    "test_identity = None\n",
    "train_identity = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EJ__pkn5JChv"
   },
   "outputs": [],
   "source": [
    "t1 = X_test.index\n",
    "y_train = X_train[\"isFraud\"].copy()\n",
    "X_train = X_train.drop(columns=[\"isFraud\"])\n",
    "X_test.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LBI77PYEJFsb"
   },
   "outputs": [],
   "source": [
    "for column in [\n",
    "    \"ProductCD\", \"card4\", \"card6\", \"R_emaildomain\", \"P_emaildomain\",\n",
    "    \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"M8\", \"M9\",\n",
    "    \"id_12\", \"id_15\", \"id_16\", \"id_23\", \"id_27\", \"id_28\", \"id_29\", \n",
    "    \"id_30\", \"id_31\", \"id_33\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \n",
    "    \"id_38\", \"DeviceType\", \"DeviceInfo\"\n",
    "]:\n",
    "    X_train[column] = X_train[column].astype(\"category\")\n",
    "    X_test[column] = X_test[column].astype(\"category\")\n",
    "factorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransactionDT seems to be in seconds, we can transform to days. 86400 is the number of seconds in a day. Similarly, transform timedeltas into actually dates. Given that D1 - D15 are time deltas, we can transform them in actual dates. This means that they are invariant to transaction date and will potentially help identify cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mpnTYIxLJTmG"
   },
   "outputs": [],
   "source": [
    "X_train[\"TDay\"] = (X_train[\"TransactionDT\"]/86400).astype(int)\n",
    "X_test[\"TDay\"] = (X_test[\"TransactionDT\"]/86400).astype(int)\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 10, 11]:\n",
    "    X_train[f\"D{i}\"] = X_train[\"TDay\"] - X_train[f\"D{i}\"]\n",
    "    X_test[f\"D{i}\"] = X_test[\"TDay\"] - X_test[f\"D{i}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting transaction amount into cents and whole value. This might be a proxy for foreign currency, as generally  fractional values are not that common in prices, but due to currency exchange prices will become fractional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FfHfXJn_XhS_"
   },
   "outputs": [],
   "source": [
    "X_train[\"cents\"] = (\n",
    "    X_train[\"TransactionAmt\"] - np.floor(X_train[\"TransactionAmt\"])).astype(\"float32\")\n",
    "X_test[\"cents\"] = (\n",
    "    X_test[\"TransactionAmt\"] - np.floor(X_test[\"TransactionAmt\"])).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "- frequency encode features. Eg. low probability of particular email domains might be associated with high fraud probability.\n",
    "- Concatenate columns together. Eg. All transactions done with card1 from a certain addr1 with a P_emaildomain might lead to fraud.\n",
    "- Group by mean - The mean transaction amount and standard deviation could lead to suspicious transactions as well. If transactions on a card have 0 standard deviation, it means that the same purchase is being made again and again. This could be suspicious because somebody might try to bypass a maximum amount spent before it triggers some extra security checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CVZ_R4YKXkVU"
   },
   "outputs": [],
   "source": [
    "frequency_encode(X_train, X_test, [\"addr1\", \"card1\", \"card2\", \"card3\", \"P_emaildomain\"])\n",
    "concatenate(X_train, X_test, \"card1\", \"addr1\")\n",
    "concatenate(X_train, X_test, \"card1_addr1\", \"P_emaildomain\")\n",
    "frequency_encode(X_train, X_test, [\"card1_addr1\", \"card1_addr1_P_emaildomain\"])\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1\", \"card1_addr1\", \"card1_addr1_P_emaildomain\"],\n",
    "    [\"TransactionAmt\", \"D9\", \"D11\"], \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1\", \"card1_addr1\", \"card1_addr1_P_emaildomain\"],\n",
    "    [\"TransactionAmt\", \"D9\", \"D11\"], \"std\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniquely identify cards and engineer more features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Gt7DMch7Xnus"
   },
   "outputs": [],
   "source": [
    "### Define card identifier \n",
    "X_train[\"D1_rounded\"] = np.floor(X_train[\"D1\"]).astype(\"str\")\n",
    "X_test[\"D1_rounded\"] = np.floor(X_test[\"D1\"]).astype(\"str\")\n",
    "concatenate(X_train, X_test, \"card1_addr1\", \"D1_rounded\")\n",
    "frequency_encode(X_train, X_test, [\"card1_addr1_D1_rounded\"])\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [\"TransactionAmt\",\"D4\", \"D9\", \"D10\", \"D15\"],\n",
    "    \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [\"TransactionAmt\",\"D4\", \"D9\", \"D10\", \"D15\"],\n",
    "    \"std\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [f\"C{i}\" for i in range(1, 15) if i != 3],\n",
    "    \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [f\"M{i}\" for i in range(1, 10)],\n",
    "    \"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folds. The start date is taken from Kaggle as it was mentioned in one of the notebooks as the potential start date of the dataset. Given this, we can find the month of each transaction. Train contains months 1-8 while test contains 9+. That means that we will use past information to predict further fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cGbD1NBKN0Ns"
   },
   "outputs": [],
   "source": [
    "START_DATE = datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "\n",
    "X_train[\"Fold\"] = X_train[\"TDay\"].apply(lambda x: START_DATE + timedelta(x))\n",
    "X_test[\"Fold\"] = X_test[\"TDay\"].apply(lambda x: START_DATE + timedelta(x))\n",
    "\n",
    "# Number of months passed since the reference point. Eg. December will be month 1\n",
    "X_train[\"Fold\"] = (\n",
    "    X_train[\"Fold\"].dt.year - START_DATE.year) * 12 + X_train[\"Fold\"].dt.month - 11\n",
    "X_test[\"Fold\"] = (\n",
    "    X_test[\"Fold\"].dt.year - START_DATE.year) * 12 + X_test[\"Fold\"].dt.month - 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features based on counts. The hypothesis here is that transactions that have the lots of domain addresses are suspicious (you're generally using one email address for most of your payments). Similar to the distances, unlikely that you make lots of transactions from different locations around the world (or at least the majority of people)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KO8V6Dd4Xp2m"
   },
   "outputs": [],
   "source": [
    "groupby_unique(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    [\"P_emaildomain\", \"dist1\", \"cents\", \"id_02\", \"C13\", \"Fold\",\n",
    "     \"V127\", \"V136\", \"V309\", \"V307\", \"V314\", \"V320\"],\n",
    "    \"card1_addr1_D1_rounded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9XybFc2hXsif"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=[\"card1_addr1_D1_rounded\", \"D1\"])\n",
    "X_train = X_train.drop(columns=[\"card1_addr1_D1_rounded\", \"D1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4Q0fP4ZKYLiL"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"TransactionDT\", \"TDay\", \"D1_rounded\", \"card4\"])\n",
    "X_test = X_test.drop(columns=[\"TransactionDT\", \"TDay\", \"D1_rounded\", \"card4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gUJu4lT_YPAj"
   },
   "outputs": [],
   "source": [
    "# Dropping some V columns that are not relevant based on:\n",
    "# Using https://www.kaggle.com/code/cdeotte/eda-for-columns-v-and-id/notebook \n",
    "# Analysing them would take way too long\n",
    "# Basically, the easiest thing to do is to:\n",
    "# - fill NAs\n",
    "# - try to run PCA\n",
    "# - do dimensionality reduction and use those features\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "v += [96, 98, 99, 104]\n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123]\n",
    "v += [124, 127, 129, 130, 136]\n",
    "v += [138, 139, 142, 147, 156, 162]\n",
    "v += [165, 160, 166]\n",
    "v += [178, 176, 173, 182]\n",
    "v += [187, 203, 205, 207, 215]\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209]\n",
    "v += [218, 223, 224, 226, 228, 229, 235]\n",
    "v += [240, 258, 257, 253, 252, 260, 261]\n",
    "v += [264, 266, 267, 274, 277]\n",
    "v += [220, 221, 234, 238, 250, 271]\n",
    "v += [294, 284, 285, 286, 291, 297]\n",
    "v += [303, 305, 307, 309, 310, 320]\n",
    "v += [281, 283, 289, 296, 301, 314]\n",
    "\n",
    "v = set(v)\n",
    "to_drop = [f\"V{i}\" for i in range(1, 340) if i not in v]\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop some of the id columns that don't have any prediction power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sKJjwot-YRHh"
   },
   "outputs": [],
   "source": [
    "id_columns_to_keep = set([\"id_01\", \"id_02\", \"id_03\", \"id_04\",\n",
    "       \"id_05\", \"id_06\", \"id_09\", \"id_10\", \"id_11\", \"id_12\", \"id_13\",\n",
    "       \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \"id_28\",\n",
    "       \"id_29\", \"id_31\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]) \n",
    "all_id_columns = set(\n",
    "    [col for col in list(X_train.columns) if col.startswith(\"id_\") and len(col) == 5]\n",
    ")\n",
    "to_drop = all_id_columns - id_columns_to_keep\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D6, D7, D8, D9, D12, D13, D14 having > 50% of values NA, will drop them completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eGi8qEpAJlZq"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])\n",
    "X_test = X_test.drop(columns=[\"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "This is using K-fold cross validation. The model is trained 6 times, each time one month is being left out. The predictions are averaged between the 6 iterations. The histograms of predictions are plotted as a mean to verify if the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiISRt8lincn",
    "outputId": "dee98edd-a71a-4cd9-904c-94c5603790e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.81451\n",
      "[100]\tvalidation_0-auc:0.89858\n",
      "[200]\tvalidation_0-auc:0.91809\n",
      "[300]\tvalidation_0-auc:0.92315\n",
      "[400]\tvalidation_0-auc:0.92489\n",
      "[500]\tvalidation_0-auc:0.92682\n",
      "[600]\tvalidation_0-auc:0.92691\n",
      "[700]\tvalidation_0-auc:0.92718\n",
      "[800]\tvalidation_0-auc:0.92698\n",
      "[900]\tvalidation_0-auc:0.92747\n",
      "[1000]\tvalidation_0-auc:0.92777\n",
      "[1100]\tvalidation_0-auc:0.92844\n",
      "[1200]\tvalidation_0-auc:0.92893\n",
      "[1300]\tvalidation_0-auc:0.92924\n",
      "[1400]\tvalidation_0-auc:0.92931\n",
      "[1500]\tvalidation_0-auc:0.92949\n",
      "[1600]\tvalidation_0-auc:0.92950\n",
      "[1700]\tvalidation_0-auc:0.92959\n",
      "[1800]\tvalidation_0-auc:0.92976\n",
      "[1900]\tvalidation_0-auc:0.92983\n",
      "[2000]\tvalidation_0-auc:0.92996\n",
      "[2100]\tvalidation_0-auc:0.92992\n",
      "[2200]\tvalidation_0-auc:0.92990\n",
      "[2300]\tvalidation_0-auc:0.92990\n",
      "[2322]\tvalidation_0-auc:0.93000\n",
      "Fold 1\n",
      "[0]\tvalidation_0-auc:0.84874\n",
      "[100]\tvalidation_0-auc:0.92418\n",
      "[200]\tvalidation_0-auc:0.94779\n",
      "[300]\tvalidation_0-auc:0.95636\n",
      "[400]\tvalidation_0-auc:0.95888\n",
      "[500]\tvalidation_0-auc:0.95958\n",
      "[600]\tvalidation_0-auc:0.95958\n",
      "[700]\tvalidation_0-auc:0.95973\n",
      "[800]\tvalidation_0-auc:0.95966\n",
      "[900]\tvalidation_0-auc:0.95944\n",
      "[909]\tvalidation_0-auc:0.95937\n",
      "Fold 2\n",
      "[0]\tvalidation_0-auc:0.84134\n",
      "[100]\tvalidation_0-auc:0.92016\n",
      "[200]\tvalidation_0-auc:0.94515\n",
      "[300]\tvalidation_0-auc:0.95588\n",
      "[400]\tvalidation_0-auc:0.95900\n",
      "[500]\tvalidation_0-auc:0.95987\n",
      "[600]\tvalidation_0-auc:0.96016\n",
      "[700]\tvalidation_0-auc:0.96047\n",
      "[800]\tvalidation_0-auc:0.96058\n",
      "[900]\tvalidation_0-auc:0.96084\n",
      "[1000]\tvalidation_0-auc:0.96099\n",
      "[1100]\tvalidation_0-auc:0.96110\n",
      "[1200]\tvalidation_0-auc:0.96096\n",
      "[1287]\tvalidation_0-auc:0.96091\n",
      "Fold 3\n",
      "[0]\tvalidation_0-auc:0.83868\n",
      "[100]\tvalidation_0-auc:0.91099\n",
      "[200]\tvalidation_0-auc:0.93854\n",
      "[300]\tvalidation_0-auc:0.94867\n",
      "[400]\tvalidation_0-auc:0.95271\n",
      "[500]\tvalidation_0-auc:0.95350\n",
      "[600]\tvalidation_0-auc:0.95322\n",
      "[693]\tvalidation_0-auc:0.95296\n",
      "Fold 4\n",
      "[0]\tvalidation_0-auc:0.86918\n",
      "[100]\tvalidation_0-auc:0.93669\n",
      "[200]\tvalidation_0-auc:0.95808\n",
      "[300]\tvalidation_0-auc:0.96586\n",
      "[400]\tvalidation_0-auc:0.96745\n",
      "[500]\tvalidation_0-auc:0.96775\n",
      "[600]\tvalidation_0-auc:0.96757\n",
      "[700]\tvalidation_0-auc:0.96741\n",
      "[719]\tvalidation_0-auc:0.96749\n",
      "Fold 5\n",
      "[0]\tvalidation_0-auc:0.83115\n",
      "[100]\tvalidation_0-auc:0.92692\n",
      "[200]\tvalidation_0-auc:0.95443\n",
      "[300]\tvalidation_0-auc:0.96527\n",
      "[400]\tvalidation_0-auc:0.96901\n",
      "[500]\tvalidation_0-auc:0.96985\n",
      "[600]\tvalidation_0-auc:0.97003\n",
      "[700]\tvalidation_0-auc:0.97005\n",
      "[746]\tvalidation_0-auc:0.97003\n"
     ]
    }
   ],
   "source": [
    "kfold = GroupKFold(n_splits=6)\n",
    "predictions = np.zeros(len(X_test))\n",
    "out_of_fold = np.zeros(len(X_train))\n",
    "columns = list(X_train.columns)\n",
    "columns.remove(\"Fold\")\n",
    "\n",
    "for counter, (index_train, index_validation) in enumerate(\n",
    "    kfold.split(X_train, y_train, groups=X_train[\"Fold\"])\n",
    "):\n",
    "    print(f\"Fold {counter}\")\n",
    "    model = xgb.XGBClassifier( \n",
    "        n_estimators=5000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='auc',\n",
    "        tree_method=\"gpu_hist\"\n",
    "    )\n",
    "    \n",
    "    res = model.fit(\n",
    "        X_train[columns].iloc[index_train], y_train.iloc[index_train],\n",
    "        eval_set=[\n",
    "            (X_train[columns].iloc[index_validation], y_train.iloc[index_validation])],\n",
    "        verbose=100, early_stopping_rounds=200\n",
    "    )\n",
    "    out_of_fold[index_validation] = model.predict_proba(\n",
    "        X_train[columns].iloc[index_validation])[:, 1]\n",
    "    predictions += model.predict_proba(X_test[columns])[:,1] / 6\n",
    "print ('XGB96 OOF CV=',roc_auc_score(y_train, out_of_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(predictions, bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('Submission histogram')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(out_of_fold, bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('Out of fold histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False).iloc[:30])\n",
    "plt.title('Most important features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QdIK0eLToD98"
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"/content/drive/My Drive/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eluHBcsioNxr"
   },
   "outputs": [],
   "source": [
    "result[\"isFraud\"] = predictions\n",
    "\n",
    "result.to_csv(\"/content/drive/My Drive/results_gpu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6a7FKa_hpyf",
    "outputId": "3b312258-af46-420f-f32e-f7f8c79e7619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  3 12:08:50 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P0    34W / 250W |    331MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DadwEV4jvSo",
    "outputId": "cb97842b-a658-4fe8-cb64-6a27617456e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost -U"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxAfs+3Jbh7UWDIdg/OdqC",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "fraud_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

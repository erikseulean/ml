{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1018f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erik/everything/venvs/ml/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba2c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(train, test, column):\n",
    "    all_labels, _ = pd.concat([train[column], test[column]], axis=0).factorize(sort=True)\n",
    "    \n",
    "    train[column] = all_labels[:len(train)].astype(int)\n",
    "    test[column] = all_labels[len(train):].astype(int)\n",
    "\n",
    "def frequency_encode(train, test, columns):\n",
    "    # Apparently this gives good results in lots of kaggle competitions\n",
    "    # I couldn't find much online why the frequency is actually useful\n",
    "    # as a feature. My intuition is that the model might match low or\n",
    "    # high frequencies with card fraud in a case or another. Eg. if \n",
    "    # most of the frauds are coming from a certain email domain, the\n",
    "    # frequency is going to be a useful feature.\n",
    "    n = len(train) + len(test)\n",
    "    for column in columns:\n",
    "        f = (pd.concat(\n",
    "                [train[column], test[column]], axis=0\n",
    "            ).value_counts(dropna=True)/n).to_dict()    \n",
    "        train[f\"{column}_FE\"] = train[column].map(f).astype(float)\n",
    "        test[f\"{column}_FE\"] = test[column].map(f).astype(float)\n",
    "\n",
    "def concatenate(train, test, col1, col2):\n",
    "    new_name = f\"{col1}_{col2}\"\n",
    "    train[new_name] = train[col1].astype(str)+ \"_\" + train[col2].astype(str)\n",
    "    test[new_name] = test[col1].astype(str) + \"_\" + test[col2].astype(str)\n",
    "    \n",
    "    encode_labels(train, test, new_name)\n",
    "    \n",
    "\n",
    "def groupby(train, test, indexes, columns, aggregation):\n",
    "    # We want for example, the mean transaction value for every card1_address1 pair\n",
    "    for index in indexes:\n",
    "        for column in columns:\n",
    "            new_name = f\"{column}_{index}_{aggregation}\"\n",
    "\n",
    "            f = pd.concat([train[[index, column]], test[[index, column]]])\n",
    "            f = f.groupby([index])[column].agg(\n",
    "                [aggregation]\n",
    "            ).reset_index().rename(columns={aggregation: new_name})\n",
    "            f.index = list(f[index])\n",
    "            f = f[new_name].to_dict()\n",
    "            train[new_name] = train[index].map(f).astype(float)\n",
    "            test[new_name] = test[index].map(f).astype(float)\n",
    "            train[new_name].fillna(-1, inplace=True)\n",
    "            test[new_name].fillna(-1, inplace=True)\n",
    "\n",
    "def groupby_unique(train, test, columns, card_id):\n",
    "    # every card_id identifies a card\n",
    "    # but generally a card_id does transactions only from a \n",
    "    # handful of email addresses, locations and details as \n",
    "    # such. So it's worth creating features that contain\n",
    "    # the number of unique email addresses, addresses, distances, etc\n",
    "    \n",
    "    for column in columns:\n",
    "        f = pd.concat([train[[card_id, column]], test[[card_id, column]]], axis=0)\n",
    "        grouped = f.groupby(card_id)[column].agg([\"nunique\"])[\"nunique\"].to_dict()\n",
    "        train[f\"{column}_{card_id}_count\"] = train[card_id].map(grouped).astype(\"float32\")\n",
    "        test[f\"{column}_{card_id}_count\"] = test[card_id].map(grouped).astype(\"float32\")\n",
    "\n",
    "\n",
    "def factorize(train, test):    \n",
    "    for column in test.columns:\n",
    "        if str(train[column].dtype) not in {\"category\", \"object\"}:\n",
    "            continue\n",
    "        encode_labels(train, test, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81c7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\n",
    "    \"../ieee_fraud_data/train_transaction.csv\", index_col=\"TransactionID\")\n",
    "X_test = pd.read_csv(\n",
    "    \"../ieee_fraud_data/test_transaction.csv\", index_col=\"TransactionID\")\n",
    "test_identity = pd.read_csv(\n",
    "    \"../ieee_fraud_data/test_identity.csv\", index_col=\"TransactionID\")\n",
    "train_identity = pd.read_csv(\n",
    "    \"../ieee_fraud_data/train_identity.csv\", index_col=\"TransactionID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbdc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "X_test = X_test.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# this does decref so GC can pick them up\n",
    "# helps having more free ram and less swap on disk\n",
    "test_identity = None\n",
    "train_identity = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58965ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = X_test.index\n",
    "y_train = X_train[\"isFraud\"].copy()\n",
    "X_train = X_train.drop(columns=[\"isFraud\"])\n",
    "X_test.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ad8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\n",
    "    \"ProductCD\", \"card4\", \"card6\", \"R_emaildomain\", \"P_emaildomain\",\n",
    "    \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"M7\", \"M8\", \"M9\",\n",
    "    \"id_12\", \"id_15\", \"id_16\", \"id_23\", \"id_27\", \"id_28\", \"id_29\", \n",
    "    \"id_30\", \"id_31\", \"id_33\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \n",
    "    \"id_38\", \"DeviceType\", \"DeviceInfo\"\n",
    "]:\n",
    "    X_train[column] = X_train[column].astype(\"category\")\n",
    "    X_test[column] = X_test[column].astype(\"category\")\n",
    "factorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9687e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TransactionDT seems to be in seconds, we can transform to days\n",
    "X_train[\"TDay\"] = (X_train[\"TransactionDT\"]/86400).astype(int)\n",
    "X_test[\"TDay\"] = (X_test[\"TransactionDT\"]/86400).astype(int)\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 10, 11]:\n",
    "    X_train[f\"D{i}\"] = X_train[\"TDay\"] - X_train[f\"D{i}\"]\n",
    "    X_test[f\"D{i}\"] = X_test[\"TDay\"] - X_test[f\"D{i}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bf068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[f\"D{i}\" for i in range(1, 16)]].isna().sum()/len(X_train)\n",
    "#D6, D7, D8, D9, D12, D13, D14 having > 50% of values NA, will drop them completely\n",
    "X_train = X_train.drop(columns=[\"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])\n",
    "X_test = X_test.drop(columns=[\"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.fillna(-9999,inplace=True)\n",
    "# X_test.fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2dd386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting transaction amount into cents and whole value\n",
    "## This might be a proxy for foreign currency, as generally\n",
    "## fractional values are not that common in prices, but due\n",
    "## to currency exchange prices it will become fractional\n",
    "X_train[\"cents\"] = (\n",
    "    X_train[\"TransactionAmt\"] - np.floor(X_train[\"TransactionAmt\"])).astype(\"float32\")\n",
    "X_test[\"cents\"] = (\n",
    "    X_test[\"TransactionAmt\"] - np.floor(X_test[\"TransactionAmt\"])).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fc2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encode(X_train, X_test, [\"addr1\", \"card1\", \"card2\", \"card3\", \"P_emaildomain\"])\n",
    "concatenate(X_train, X_test, \"card1\", \"addr1\")\n",
    "concatenate(X_train, X_test, \"card1_addr1\", \"P_emaildomain\")\n",
    "frequency_encode(X_train, X_test, [\"card1_addr1\", \"card1_addr1_P_emaildomain\"])\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1\", \"card1_addr1\", \"card1_addr1_P_emaildomain\"],\n",
    "    [\"TransactionAmt\", \"D11\"], \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1\", \"card1_addr1\", \"card1_addr1_P_emaildomain\"],\n",
    "    [\"TransactionAmt\", \"D11\"], \"std\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9455b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define card identifier \n",
    "X_train[\"D1_rounded\"] = np.floor(X_train[\"D1\"]).astype(\"str\")\n",
    "X_test[\"D1_rounded\"] = np.floor(X_test[\"D1\"]).astype(\"str\")\n",
    "concatenate(X_train, X_test, \"card1_addr1\", \"D1_rounded\")\n",
    "frequency_encode(X_train, X_test, [\"card1_addr1_D1_rounded\"])\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [\"TransactionAmt\",\"D4\", \"D10\", \"D15\"],\n",
    "    \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [\"TransactionAmt\",\"D4\", \"D10\", \"D15\"],\n",
    "    \"std\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [f\"C{i}\" for i in range(1, 15)],\n",
    "    \"mean\"\n",
    ")\n",
    "groupby(\n",
    "    X_train, X_test,\n",
    "    [\"card1_addr1_D1_rounded\"],\n",
    "    [f\"M{i}\" for i in range(1, 10)],\n",
    "    \"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c60b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_unique(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    [\"P_emaildomain\", \"dist1\", \"cents\", \"id_02\", \"C13\",\n",
    "     \"V127\", \"V136\", \"V309\", \"V307\", \"V314\", \"V320\"],\n",
    "    \"card1_addr1_D1_rounded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7f8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=[\"card1_addr1_D1_rounded\"])\n",
    "X_train = X_train.drop(columns=[\"card1_addr1_D1_rounded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926dc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[X_train[\"P_emaildomain_card1_addr1_D1_count\"] > 3][\n",
    "#     [\n",
    "#         \"card1_addr1_D1\", \"card1\", \"P_emaildomain\",\n",
    "#         \"P_emaildomain_card1_addr1_D1_count\"]\n",
    "#     ].sort_values(by=[\"card1_addr1_D1\"])\n",
    "# X_train[X_train[\"card1_addr1_D1\"] == 82][\n",
    "#     [\n",
    "#         \"card1_addr1_D1\", \"card1\", \"addr1\", \"P_emaildomain\", \n",
    "#         \"TransactionAmt\", \"P_emaildomain_card1_addr1_D1_count\"]\n",
    "#     ].sort_values(by=[\"card1_addr1_D1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9280b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction folds\n",
    "# Given that TDay takes values between 1 and 183 it means that \n",
    "# we have roughly 6 months of data (to train on). For the test\n",
    "# dataset we have dates in the future.\n",
    "\n",
    "# We can do CV by holding out one month, train on the other months\n",
    "# and repeat and so on. This makes sure that it's trained on months\n",
    "# different than it is tested on, to simmulate what we have in the\n",
    "# test datset\n",
    "\n",
    "# This is considered the reference starting point of the transactions.\n",
    "# (Took it from some comments)\n",
    "START_DATE = datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "\n",
    "X_train[\"Fold\"] = X_train[\"TDay\"].apply(lambda x: START_DATE + timedelta(x))\n",
    "X_test[\"Fold\"] = X_test[\"TDay\"].apply(lambda x: START_DATE + timedelta(x))\n",
    "\n",
    "# Number of months passed since the reference point. Eg. December will be month 1\n",
    "X_train[\"Fold\"] = (\n",
    "    X_train[\"Fold\"].dt.year - START_DATE.year) * 12 + X_train[\"Fold\"].dt.month - 11\n",
    "X_test[\"Fold\"] = (\n",
    "    X_test[\"Fold\"].dt.year - START_DATE.year) * 12 + X_test[\"Fold\"].dt.month - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4ad242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"TransactionDT\", \"TDay\", \"D1_rounded\"])\n",
    "X_test = X_test.drop(columns=[\"TransactionDT\", \"TDay\", \"D1_rounded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f17644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some V columns that are not relevant based on:\n",
    "# Using https://www.kaggle.com/code/cdeotte/eda-for-columns-v-and-id/notebook \n",
    "# Analysing them would take way too long\n",
    "# Basically, the easiest thing to do is to:\n",
    "# - fill NAs\n",
    "# - try to run PCA\n",
    "# - do dimensionality reduction and use those features\n",
    "v =  [1, 3, 4, 6, 8, 11]\n",
    "v += [13, 14, 17, 20, 23, 26, 27, 30]\n",
    "v += [36, 37, 40, 41, 44, 47, 48]\n",
    "v += [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "v += [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "v += [96, 98, 99, 104]\n",
    "v += [107, 108, 111, 115, 117, 120, 121, 123]\n",
    "v += [124, 127, 129, 130, 136]\n",
    "v += [138, 139, 142, 147, 156, 162]\n",
    "v += [165, 160, 166]\n",
    "v += [178, 176, 173, 182]\n",
    "v += [187, 203, 205, 207, 215]\n",
    "v += [169, 171, 175, 180, 185, 188, 198, 210, 209]\n",
    "v += [218, 223, 224, 226, 228, 229, 235]\n",
    "v += [240, 258, 257, 253, 252, 260, 261]\n",
    "v += [264, 266, 267, 274, 277]\n",
    "v += [220, 221, 234, 238, 250, 271]\n",
    "v += [294, 284, 285, 286, 291, 297]\n",
    "v += [303, 305, 307, 309, 310, 320]\n",
    "v += [281, 283, 289, 296, 301, 314]\n",
    "\n",
    "v = set(v)\n",
    "to_drop = [f\"V{i}\" for i in range(1, 340) if i not in v]\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f14024d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns_to_keep = set([\"id_01\", \"id_02\", \"id_03\", \"id_04\",\n",
    "       \"id_05\", \"id_06\", \"id_09\", \"id_10\", \"id_11\", \"id_12\", \"id_13\",\n",
    "       \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \"id_28\",\n",
    "       \"id_29\", \"id_31\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]) \n",
    "all_id_columns = set(\n",
    "    [col for col in list(X_train.columns) if col.startswith(\"id_\") and len(col) == 5]\n",
    ")\n",
    "to_drop = all_id_columns - id_columns_to_keep\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9442d64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kfold = GroupKFold(n_splits=6)\n",
    "predictions = np.zeros(len(X_test))\n",
    "out_of_fold = np.zeros(len(X_train))\n",
    "columns = list(X_train.columns)\n",
    "columns.remove(\"Fold\")\n",
    "\n",
    "for counter, (index_train, index_validation) in enumerate(kfold.split(X_train, y_train, groups=X_train[\"Fold\"])):\n",
    "    print(f\"Fold {counter}\")\n",
    "    model = xgb.XGBClassifier( \n",
    "        n_estimators=2000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.02,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    \n",
    "    res = model.fit(\n",
    "        X_train[columns].iloc[index_train], y_train.iloc[index_train],\n",
    "        eval_set=[(X_train[columns].iloc[index_validation], y_train.iloc[index_validation])],\n",
    "        verbose=100, early_stopping_rounds=200\n",
    "    )\n",
    "    out_of_fold[index_validation] = model.predict_proba(X_train[columns].iloc[index_validation])[:, 1]\n",
    "    predictions += model.predict_proba(X_test[columns])[:,1] / 6 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0f11811",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxT = X_train.index[:3*len(X_train)//4]\n",
    "idxV = X_train.index[3*len(X_train)//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d0d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier( \n",
    "    n_estimators=2000,\n",
    "    max_depth=12, \n",
    "    learning_rate=0.02,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.4,\n",
    "    missing=-1, \n",
    "    eval_metric='auc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10022317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erik/everything/venvs/ml/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/erik/everything/venvs/ml/lib/python3.9/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.77592\n",
      "[50]\tvalidation_0-auc:0.88552\n",
      "[100]\tvalidation_0-auc:0.90306\n",
      "[150]\tvalidation_0-auc:0.92057\n",
      "[200]\tvalidation_0-auc:0.93415\n",
      "[250]\tvalidation_0-auc:0.94096\n",
      "[300]\tvalidation_0-auc:0.94468\n",
      "[350]\tvalidation_0-auc:0.94686\n",
      "[400]\tvalidation_0-auc:0.94785\n",
      "[450]\tvalidation_0-auc:0.94846\n",
      "[500]\tvalidation_0-auc:0.94865\n",
      "[550]\tvalidation_0-auc:0.94836\n",
      "[596]\tvalidation_0-auc:0.94839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4,\n",
       "              enable_categorical=False, eval_metric='auc', gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=-1, monotone_constraints='()',\n",
       "              n_estimators=2000, n_jobs=1, num_parallel_tree=1,\n",
       "              predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=0.8, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.loc[idxT], y_train[idxT], \n",
    "        eval_set=[(X_train.loc[idxV],y_train[idxV])],\n",
    "        verbose=50, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32ab3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"isFraud\": model.predict_proba(X_test)[:,1], \"TransactionId\": t1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adcac337",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(\"result_fixed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prediction[\"isFraud\"],bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('XGB96 Submission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8762ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out_of_fold,bins=100)\n",
    "plt.ylim((0,5000))\n",
    "plt.title('XGB OOF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n",
    "plt.title('XGB95 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15565f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../ieee_fraud_data/sample_submission.csv\")\n",
    "submission.isFraud = predictions\n",
    "submission.to_csv(\"xgb_cv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb779899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bef3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions = pd.read_csv(\"../ieee_fraud_data/train_transaction.csv\")\n",
    "test_transactions = pd.read_csv(\"../ieee_fraud_data/test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ce4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(\"../ieee_fraud_data/test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t1[\"TransactionID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6279ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TransactionDT seems to be in seconds, we can transform to days\n",
    "train_transactions[\"TransactionDay\"] = (train_transactions[\"TransactionDT\"]/86400).astype(int)\n",
    "test_transactions[\"TransactionDay\"] = (test_transactions[\"TransactionDT\"]/86400).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f304c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_transactions[\"isFraud\"].copy()\n",
    "train_transactions = train_transactions.drop(columns=[\"isFraud\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a03ba",
   "metadata": {},
   "source": [
    "##### What features to drop first ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aaee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions[[\"dist1\", \"dist2\"]].isna().sum()/len(train_transactions)\n",
    "# This yields dist1, dist2 having > 50% of values NA, will drop them completely\n",
    "train_transactions = train_transactions.drop(columns=[\"dist1\", \"dist2\"])\n",
    "test_transactions = test_transactions.drop(columns=[\"dist1\", \"dist2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a74f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions[[f\"M{i}\" for i in range(1, 10)]].isna().sum()/len(train_transactions)\n",
    "# This yields M7, M8, M9 having > 50% of values NA, will drop them completely\n",
    "train_transactions = train_transactions.drop(columns=[\"M7\", \"M8\", \"M9\"])\n",
    "test_transactions = test_transactions.drop(columns=[\"M7\", \"M8\", \"M9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions[[f\"D{i}\" for i in range(1, 16)]].isna().sum()/len(train_transactions)\n",
    "# This yields D5, D6, D7, D8, D9, D12, D13, D14 having > 50% of values NA, will drop them completely\n",
    "train_transactions = train_transactions.drop(columns=[\"D5\", \"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])\n",
    "test_transactions = test_transactions.drop(columns=[\"D5\", \"D6\", \"D7\", \"D8\", \"D9\", \"D12\", \"D13\", \"D14\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db06649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V* columns are too many and would take ages to figure things out. First thing we could try is\n",
    "# to try running PCA and reduce the dimension, but will refrain from this unless the rest of the\n",
    "# features are completely useless\n",
    "train_transactions = train_transactions.drop(columns=[f\"V{i}\" for i in range(1, 340)])\n",
    "test_transactions = test_transactions.drop(columns=[f\"V{i}\" for i in range(1, 340)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ee408",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c287f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(train, test, column):\n",
    "    all_labels, _ = pd.concat([train[column], test[column]], axis=0).factorize(sort=True)\n",
    "    \n",
    "    train[column] = all_labels[:len(train)].astype(int)\n",
    "    test[column] = all_labels[len(train):].astype(int)\n",
    "\n",
    "def frequency_encode(train, test, columns):\n",
    "    # Apparently this gives good results in lots of kaggle competitions\n",
    "    # I couldn't find much online why the frequency is actually useful\n",
    "    # as a feature. My intuition is that the model might match low or\n",
    "    # high frequencies with card fraud in a case or another. Eg. if \n",
    "    # most of the frauds are coming from a certain email domain, the\n",
    "    # frequency is going to be a useful feature.\n",
    "    n = len(train) + len(test)\n",
    "    for column in columns:\n",
    "        f = (pd.concat(\n",
    "                [train[column], test[column]], axis=0\n",
    "            ).value_counts(dropna=True)/n).to_dict()    \n",
    "        train[f\"{column}_FE\"] = train[column].map(f).astype(float)\n",
    "        test[f\"{column}_FE\"] = test[column].map(f).astype(float)\n",
    "\n",
    "def concatenate(train, test, col1, col2):\n",
    "    new_name = f\"{col1}_{col2}\"\n",
    "    train[new_name] = train[col1].astype(str)+ \"_\" + train[col2].astype(str)\n",
    "    test[new_name] = test[col1].astype(str) + \"_\" + test[col2].astype(str)\n",
    "    \n",
    "    encode_labels(train, test, new_name)\n",
    "    \n",
    "\n",
    "def groupby(train, test, index, column, aggregation):\n",
    "    # We want for example, the mean transaction value for every card1_address1 pair\n",
    "    new_name = f\"{column}_{index}_{aggregation}\"\n",
    "    \n",
    "    f = pd.concat([train[[index, column]], test[[index, column]]])\n",
    "    f = f.groupby([index])[column].agg([aggregation]).reset_index().rename(columns={aggregation: new_name})\n",
    "    f.index = list(f[index])\n",
    "    f = f[new_name].to_dict()\n",
    "    train[new_name] = train[index].map(f).astype(float)\n",
    "    test[new_name] = train[index].map(f).astype(float)\n",
    "    train[new_name].fillna(-1, inplace=True)\n",
    "    test[new_name].fillna(-1, inplace=True)\n",
    "\n",
    "def groupby_unique(train, test, columns, card_id):\n",
    "    # every card_id identifies a card\n",
    "    # but generally a card_id does transactions only from a \n",
    "    # handful of email addresses, locations and details as \n",
    "    # such. So it's worth creating features that contain\n",
    "    # the number of unique email addresses, addresses, distances, etc\n",
    "    \n",
    "    for column in columns:\n",
    "        f = pd.concat([train[[card_id] + [column]], test[[card_id] + [column]]], axis=0)\n",
    "        grouped = f.groupby(card_id)[column].agg([\"nunique\"])[\"nunique\"].to_dict()\n",
    "        train[f\"{column}_{card_id}_count\"] = train[card_id].map(grouped).astype(\"float32\")\n",
    "        test[f\"{column}_{card_id}_count\"] = test[card_id].map(grouped).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e693a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions[train_transactions[\"card1_addr1\"] == 13832][[\"card1\", \"TransactionAmt\", \"addr1\", \"P_emaildomain\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions = train_transactions.drop(columns=[\"TransactionID\", \"TransactionDT\"])\n",
    "test_transactions = test_transactions.drop(columns=[\"TransactionID\", \"TransactionDT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd4483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    train_transactions[f\"D{i}_norm\"] = train_transactions[\"TransactionDay\"] - train_transactions[f\"D{i}\"]\n",
    "    test_transactions[f\"D{i}_norm\"] = test_transactions[\"TransactionDay\"] - train_transactions[f\"D{i}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d554a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate(train_transactions, test_transactions, \"card1\",\"addr1\")\n",
    "concatenate(train_transactions, test_transactions, \"card1_addr1\", \"P_emaildomain\")\n",
    "frequency_encode(train_transactions, test_transactions, [\n",
    "    \"addr1\", \"card1\", \"card2\", \"card3\", \"P_emaildomain\", \"card1_addr1\", \"card1_addr1_P_emaildomain\"])\n",
    "\n",
    "groupby(train_transactions, test_transactions, \"card1\", \"TransactionAmt\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1\", \"TransactionAmt\", \"std\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1\", \"TransactionAmt\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1\", \"TransactionAmt\", \"std\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1_P_emaildomain\", \"TransactionAmt\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1_P_emaildomain\", \"TransactionAmt\", \"std\")\n",
    "\n",
    "groupby(train_transactions, test_transactions, \"card1\", \"D11\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1\", \"D11\", \"std\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1\", \"D11\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1\", \"D11\", \"std\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1_P_emaildomain\", \"D11\", \"mean\")\n",
    "groupby(train_transactions, test_transactions, \"card1_addr1_P_emaildomain\", \"D11\", \"std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858c62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate(train_transactions, test_transactions, \"card1_addr1\", \"D1_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transactions = train_transactions.drop(\n",
    "    columns=[\n",
    "        \"card1\", \"addr1\", \"P_emaildomain\", \"D1\",\n",
    "        \"card1_addr1\", \"card1_addr1_P_emaildomain\",\n",
    "        #\"card1_addr1_FE\", \"card1_addr1_P_emaildomain_FE\"\n",
    "    ]\n",
    ")\n",
    "test_transactions = test_transactions.drop(\n",
    "    columns=[\n",
    "        \"card1\", \"addr1\", \"P_emaildomain\", \"D1\",\n",
    "        \"card1_addr1\", \"card1_addr1_P_emaildomain\",\n",
    "        #\"card1_addr1_FE\", \"card1_addr1_P_emaildomain_FE\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO figure out what to do with these\n",
    "# train_transactions = train_transactions.drop(columns=[\n",
    "#     \"ProductCD\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"card5\", \"card6\"]\n",
    "# )\n",
    "# test_transactions = test_transactions.drop(columns=[\n",
    "#     \"ProductCD\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"card5\", \"card6\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d44d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxT = train_transactions.index[:3*len(train_transactions)//4]\n",
    "idxV = test_transactions.index[3*len(test_transactions)//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc = HistGradientBoostingClassifier(    \n",
    "    max_depth=12,\n",
    "    learning_rate=0.02, \n",
    ")\n",
    "hgbc.fit(train_transactions.loc[idxT], y_train[idxT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f96d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc.score(train_transactions.loc[idxV],y_train[idxV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c995ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\"isFraud\": hgbc.predict(test_transactions), \"TransactionId\": t1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b603a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3e44b",
   "metadata": {},
   "source": [
    "#### xgboost \n",
    "- faster than the GradientBoosting from sklearn\n",
    "- can deal with missing values\n",
    "- can fit directly categorical features without encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f07ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier( \n",
    "        n_estimators=2000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='auc',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "        #tree_method='gpu_hist' \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed86dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35763ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"M7\", \"M8\", \"M9\"]:\n",
    "    train_transactions[column] = label_encoder.fit_transform(train_transactions[column])\n",
    "    test_transactions[column] = label_encoder.fit_transform(test_transactions[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\"ProductCD\", \"card4\", \"card6\", \"R_emaildomain\",\n",
    "               \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \"M6\",]:\n",
    "    train_transactions[column] = label_encoder.fit_transform(train_transactions[column])\n",
    "    test_transactions[column] = label_encoder.fit_transform(test_transactions[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "train_transactions['DT_M'] = train_transactions['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "train_transactions['DT_M'] = (train_transactions['DT_M'].dt.year-2017)*12 + train_transactions['DT_M'].dt.month \n",
    "\n",
    "test_transactions['DT_M'] = test_transactions['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "test_transactions['DT_M'] = (test_transactions['DT_M'].dt.year-2017)*12 + test_transactions['DT_M'].dt.month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a13708",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(train_transactions))\n",
    "preds = np.zeros(len(test_transactions))\n",
    "\n",
    "skf = GroupKFold(n_splits=6)\n",
    "\n",
    "for i, (idxT, idxV) in enumerate( skf.split(train_transactions, y_train, groups=train_transactions['DT_M'])):\n",
    "        month = train_transactions.iloc[idxV]['DT_M'].iloc[0]\n",
    "        print('Fold',i,'withholding month',month)\n",
    "        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=5000,\n",
    "            max_depth=12,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.4,\n",
    "            missing=-1,\n",
    "            eval_metric='auc',\n",
    "            # USE CPU\n",
    "            #nthread=4,\n",
    "            #tree_method='hist'\n",
    "            # USE GPU\n",
    "            #tree_method='gpu_hist' \n",
    "        )        \n",
    "        h = clf.fit(train_transactions.iloc[idxT], y_train.iloc[idxT], \n",
    "                eval_set=[(train_transactions.iloc[idxV],y_train.iloc[idxV])],\n",
    "                verbose=100, early_stopping_rounds=200)\n",
    "    \n",
    "        oof[idxV] += clf.predict_proba(train_transactions.iloc[idxV])[:,1]\n",
    "        preds += clf.predict_proba(train_transactions)[:,1]/skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = clf.fit(train_transactions.loc[idxT], y_train[idxT], \n",
    "        eval_set=[(train_transactions.loc[idxV],y_train[idxV])],\n",
    "        verbose=50, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,train_transactions.columns)), columns=['Value','Feature'])\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n",
    "plt.title('XGB95 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c589929",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4j/nwn878q577s1s_pmzlyckl_00000gn/T/ipykernel_1590/3010029192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"isFraud\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_transactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransactionId\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = pd.DataFrame({\"isFraud\": clf.predict_proba(test_transactions), \"TransactionId\": t1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ef9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(\"result_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is given,\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "train_transactions[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee793012",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
